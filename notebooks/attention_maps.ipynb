{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa359c75",
   "metadata": {},
   "source": [
    "# NAF: Neighborhood Attention Map\n",
    "\n",
    "This notebook provides tools to inspect and visualize attention weights from the NAF model.\n",
    "\n",
    "**Features:**\n",
    "- Load pre-trained NAF model and test images\n",
    "- Select query points on the image\n",
    "- Extract and visualize attention weights for the selected position\n",
    "- Compare attention patterns across different image regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d519e9",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa00f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use inline matplotlib for better compatibility\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aaceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra.utils import instantiate\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from src.model.naf import NAF\n",
    "from utils.training import load_multiple_backbones\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2417fe5b",
   "metadata": {},
   "source": [
    "## 2. Load Model and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5139fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Hydra config\n",
    "if not GlobalHydra.instance().is_initialized():\n",
    "    initialize(config_path=\"../config\", version_base=None)\n",
    "\n",
    "cfg = compose(config_name=\"base\", overrides=[\"model=naf\", \"img_size=1024\"])\n",
    "\n",
    "# Initialize NAF model\n",
    "model = torch.hub.load(\"valeoai/NAF\", \"naf\", pretrained=True, device=device)\n",
    "model.eval()\n",
    "\n",
    "# Load backbone\n",
    "backbone_configs = [{\"name\": \"vit_base_patch16_dinov3.lvd1689m\"}]\n",
    "backbones, names, _ = load_multiple_backbones(cfg, backbone_configs, device)\n",
    "backbone = backbones[0]\n",
    "backbone.eval()\n",
    "\n",
    "print(f\"Model loaded: {type(model).__name__}\")\n",
    "print(f\"Backbone loaded: {names[0]}\")\n",
    "print(f\"Kernel size: {model.upsampler.kernel_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae72e4",
   "metadata": {},
   "source": [
    "## 3. Load and Preprocess Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae219175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "image_path = \"../asset/dog0.jpg\"\n",
    "pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Resize to model input size\n",
    "H, W = cfg.img_size, cfg.img_size\n",
    "transform = T.Compose([T.Resize((H, W)), T.ToTensor()])\n",
    "image = transform(pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "# Normalize for backbone\n",
    "mean_bck = backbone.config[\"mean\"]\n",
    "std_bck = backbone.config[\"std\"]\n",
    "normalize_bck = T.Normalize(mean=mean_bck, std=std_bck)\n",
    "image_bck = normalize_bck(image)\n",
    "\n",
    "# Normalize for upsampler (ImageNet normalization)\n",
    "normalize_ups = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "image_ups = normalize_ups(image)\n",
    "\n",
    "# Convert to display format\n",
    "image_vis = (image.clone().cpu().squeeze(0).permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "\n",
    "print(f\"Loaded image from: {image_path}\")\n",
    "print(f\"Image size: {image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457092d",
   "metadata": {},
   "source": [
    "## 4. Extract Backbone Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784c862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from backbone\n",
    "with torch.no_grad():\n",
    "    hr_feats = backbone(image_bck)\n",
    "\n",
    "# Define output size for encoder (match backbone output)\n",
    "output_size = (hr_feats.shape[2], hr_feats.shape[3])\n",
    "\n",
    "# Compute scaling factors between image and feature space\n",
    "scale_h = output_size[0] / H\n",
    "scale_w = output_size[1] / W\n",
    "\n",
    "print(f\"Backbone features shape: {hr_feats.shape}\")\n",
    "print(f\"Feature map size: {output_size}\")\n",
    "print(f\"Scaling factors - h: {scale_h:.3f}, w: {scale_w:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a80c95",
   "metadata": {},
   "source": [
    "## 5. Helper Functions for Query Position and Attention Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a3b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attention_at_position(query_img_h, query_img_w):\n",
    "    \"\"\"\n",
    "    Extract attention weights for a query position.\n",
    "    \n",
    "    Args:\n",
    "        query_img_h: Query position height in image coordinates (0-447)\n",
    "        query_img_w: Query position width in image coordinates (0-447)\n",
    "        \n",
    "    Returns:\n",
    "        attn_map: 2D attention map of shape (kernel_h, kernel_w)\n",
    "        pos_feat: Query position in feature space\n",
    "    \"\"\"\n",
    "    # Convert image coordinates to feature space\n",
    "    pos_feat = (int(query_img_h * scale_h), int(query_img_w * scale_w))\n",
    "    \n",
    "    # Extract attention weights\n",
    "    with torch.no_grad():\n",
    "        # Get RoPE-transformed features (queries)\n",
    "        _, attn_weights = model(image_ups, hr_feats, output_size=output_size, return_weights=True)\n",
    "        \n",
    "    # Extract attention for the query position\n",
    "    # attn_weights: [B, num_heads, H, W, kernel_h * kernel_w]\n",
    "    query_h, query_w = pos_feat\n",
    "    attn_at_query = attn_weights[0, :, query_h, query_w, :]  # [num_heads, kernel_h*kernel_w]\n",
    "    \n",
    "    # Average across heads\n",
    "    attn_mean = attn_at_query.mean(dim=0)  # [kernel_h*kernel_w]\n",
    "    \n",
    "    # Reshape to 2D attention map\n",
    "    kernel_h, kernel_w = model.upsampler.kernel_size\n",
    "    attn_map = attn_mean.view(kernel_h, kernel_w).cpu().numpy()\n",
    "    \n",
    "    return attn_map, pos_feat\n",
    "\n",
    "\n",
    "print(\"Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f289e60c",
   "metadata": {},
   "source": [
    "## 6. Visualize Attention for a Single Query Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd18126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive attention visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12*2/3, 4*2/3))\n",
    "\n",
    "# Initialize with default position\n",
    "query_img_h, query_img_w = 650, 270\n",
    "\n",
    "# Plot elements that will be updated\n",
    "img_display = axes[0].imshow(image_vis)\n",
    "query_point, = axes[0].plot([], [], 'r*', markersize=15, markeredgecolor='white', markeredgewidth=1)\n",
    "rect_patch = patches.Rectangle((0, 0), 0, 0, linewidth=2, edgecolor='red', facecolor='none', linestyle='--')\n",
    "axes[0].add_patch(rect_patch)\n",
    "\n",
    "axes[0].set_xlabel('Image Width')\n",
    "axes[0].set_ylabel('Image Height')\n",
    "axes[0].set_title('Click on image to select query point')\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# Attention heatmap (will be updated)\n",
    "attn_display = axes[1].imshow(np.zeros((5, 5)), cmap='RdBu_r', aspect='equal')\n",
    "axes[1].set_xlabel('Kernel Width')\n",
    "axes[1].set_ylabel('Kernel Height')\n",
    "axes[1].set_title('Attention Map')\n",
    "\n",
    "cbar1 = plt.colorbar(attn_display, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "cbar1.set_label('Attention Weight')\n",
    "\n",
    "# Overlay visualization\n",
    "overlay_display = axes[2].imshow(np.zeros((10, 10, 3), dtype=np.uint8))\n",
    "axes[2].set_xlabel('Width')\n",
    "axes[2].set_ylabel('Height')\n",
    "axes[2].set_title('Attention Overlay on Neighborhood')\n",
    "axes[2].set_aspect('equal')\n",
    "\n",
    "# Statistics text\n",
    "stats_text = axes[1].text(0.02, 0.98, '', transform=axes[1].transAxes, \n",
    "                          verticalalignment='top', fontsize=8, \n",
    "                          bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "def update_attention(query_h, query_w):\n",
    "    \"\"\"Update the attention visualization for a new query position.\"\"\"\n",
    "    # Extract attention map\n",
    "    attn_map, pos_feat = extract_attention_at_position(query_h, query_w)\n",
    "    kernel_h, kernel_w = model.upsampler.kernel_size\n",
    "    \n",
    "    # Update query point\n",
    "    query_point.set_data([query_w], [query_h])\n",
    "    \n",
    "    # Update neighborhood rectangle\n",
    "    half_h, half_w = kernel_h // 2, kernel_w // 2\n",
    "    box_x0 = max(0, query_w - int(half_w / scale_w))\n",
    "    box_y0 = max(0, query_h - int(half_h / scale_h))\n",
    "    box_x1 = min(W, query_w + int(half_w / scale_w))\n",
    "    box_y1 = min(H, query_h + int(half_h / scale_h))\n",
    "    \n",
    "    rect_patch.set_xy((box_x0, box_y0))\n",
    "    rect_patch.set_width(box_x1 - box_x0)\n",
    "    rect_patch.set_height(box_y1 - box_y0)\n",
    "    \n",
    "    # Update attention heatmap\n",
    "    attn_display.set_data(attn_map)\n",
    "    attn_display.set_clim(vmin=attn_map.min(), vmax=attn_map.max())\n",
    "    axes[1].set_title(f'Attention Map ({kernel_h}Ã—{kernel_w})')\n",
    "    \n",
    "    # Create overlay visualization\n",
    "    # Extract neighborhood from image\n",
    "    neighborhood = image_vis[box_y0:box_y1, box_x0:box_x1, :]\n",
    "    \n",
    "    # Resize attention map to match neighborhood size\n",
    "    from scipy.ndimage import zoom\n",
    "    attn_resized = zoom(attn_map, (neighborhood.shape[0] / attn_map.shape[0], \n",
    "                                    neighborhood.shape[1] / attn_map.shape[1]), order=1)\n",
    "    \n",
    "    # Normalize attention to 0-1 for overlay\n",
    "    attn_norm = (attn_resized - attn_resized.min()) / (attn_resized.max() - attn_resized.min() + 1e-8)\n",
    "    \n",
    "    # Create overlay: blend image with attention heatmap\n",
    "    overlay = neighborhood.copy().astype(float)\n",
    "    \n",
    "    # Apply colormap to attention (red-blue)\n",
    "    import matplotlib.cm as cm\n",
    "    attn_colored = cm.RdBu_r(attn_norm)[:, :, :3] * 255  # RGB only\n",
    "    \n",
    "    # Blend: 50% image, 50% attention heatmap\n",
    "    overlay_blended = (0.5 * overlay + 0.5 * attn_colored).astype(np.uint8)\n",
    "    \n",
    "    # Update overlay display\n",
    "    overlay_display.set_data(overlay_blended)\n",
    "    overlay_display.set_extent([box_x0, box_x1, box_y1, box_y0])\n",
    "    axes[2].set_xlim(box_x0, box_x1)\n",
    "    axes[2].set_ylim(box_y1, box_y0)\n",
    "    axes[2].set_title('Attention Overlay on Neighborhood')\n",
    "    \n",
    "    # Update statistics text\n",
    "    fig.canvas.draw_idle()\n",
    "    \n",
    "    # Print to console\n",
    "    print(f\"Query: ({query_h}, {query_w}) | Feature: {pos_feat} | Center attn: {attn_map[kernel_h//2, kernel_w//2]:.4f}\")\n",
    "\n",
    "def onclick(event):\n",
    "    \"\"\"Handle click events on the image.\"\"\"\n",
    "    if event.inaxes == axes[0] and event.xdata is not None and event.ydata is not None:\n",
    "        new_w = int(np.clip(event.xdata, 0, W-1))\n",
    "        new_h = int(np.clip(event.ydata, 0, H-1))\n",
    "        update_attention(new_h, new_w)\n",
    "\n",
    "fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Initial visualization\n",
    "update_attention(query_img_h, query_img_w)\n",
    "plt.show()\n",
    "\n",
    "print(\"Click on the left image to explore attention patterns!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e96127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
