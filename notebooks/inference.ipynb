{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering.\n",
    "\n",
    "In this notebook we present how to upsample **Any Feature from Any VFM at Any Resolution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import PIL\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra.utils import instantiate\n",
    "from IPython.display import clear_output\n",
    "\n",
    "project_root = str(Path().absolute().parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from utils.training import get_batch, get_dataloaders, load_multiple_backbones\n",
    "from utils.visualization import plot_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not GlobalHydra.instance().is_initialized():\n",
    "    initialize(config_path=\"../config\", version_base=None)\n",
    "\n",
    "overrides = [\"val_dataloader.batch_size=1\", \n",
    "             \"train_dataloader.batch_size=1\", \n",
    "             \"model=naf\", \n",
    "             \"img_size=448\"]\n",
    "\n",
    "cfg = compose(config_name=\"base\", overrides=overrides)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader = get_dataloaders(cfg, shuffle=True)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = torch.hub.load(\"valeoai/NAF\", \"naf\", pretrained=True, device=device)\n",
    "model.cuda()\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either iterate over the dataloader\n",
    "batch = next(iter(val_dataloader))\n",
    "batch = get_batch(batch, device)\n",
    "img_batch = batch[\"image\"]\n",
    "bs = img_batch.shape[0]\n",
    "IMG_PATH = None\n",
    "\n",
    "# Or load a custom image\n",
    "IMG_PATH = \"../asset/dinov3.png\"\n",
    "img_batch = PIL.Image.open(IMG_PATH).convert(\"RGB\")\n",
    "img_batch = val_dataloader.dataset.transform(img_batch).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def upsample_backbone(backbone, img, model, mean_std_bck, mean_std_ups, sizes=[512]):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    mean_bck, std_bck = mean_std_bck\n",
    "    mean_ups, std_ups = mean_std_ups\n",
    "\n",
    "    img_bck = T.functional.normalize(img, mean=mean_bck, std=std_bck)\n",
    "    img_ups = T.functional.normalize(img, mean=mean_ups, std=std_ups)\n",
    "\n",
    "    hr_feats = backbone(img_bck)\n",
    "    hr_size = hr_feats.shape[-1]\n",
    "    preds = []\n",
    "    for size in sizes:\n",
    "        pred = model(img_ups, hr_feats, (size, size))\n",
    "        preds.append(pred)\n",
    "\n",
    "    hr_feats_ = torch.nn.functional.interpolate(hr_feats, size, mode=\"nearest-exact\")\n",
    "    plot_feats(\n",
    "        img_batch[0],\n",
    "        hr_feats_[0],\n",
    "        [p[0] for p in preds],\n",
    "        legend=[f\"Input\", f\"{hr_size}x{hr_size}\"] + [f\"{size}x{size}\" for size in sizes],\n",
    "        font_size=20,\n",
    "    )\n",
    "    plt.show()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_configs = [\n",
    "    {\"name\": \"vit_base_patch16_dinov3.lvd1689m\"},\n",
    "    {\"name\": \"radio_v2.5-b\"},\n",
    "    {\"name\": \"franca_vitb14\"},\n",
    "    {\"name\": \"vit_base_patch14_reg4_dinov2\"},\n",
    "    {\"name\": \"vit_base_patch14_dinov2.lvd142m\"},\n",
    "]\n",
    "mapping = {\n",
    "    \"vit_base_patch16_dinov3.lvd1689m\": \"Dinov3-B\",\n",
    "    \"radio_v2.5-b\": \"Radio-v2.5-B\",\n",
    "    \"franca_vitb14\": \"Franca-B14\",\n",
    "    \"vit_base_patch14_reg4_dinov2\": \"Dinov2-R-B\",\n",
    "    \"vit_base_patch14_dinov2.lvd142m\": \"Dinov2-B\",\n",
    "}\n",
    "backbones, *_ = load_multiple_backbones(cfg, backbone_configs, device)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ups, std_ups = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
    "for backbone in backbones:\n",
    "    backbone.cuda().eval()\n",
    "\n",
    "    mean_bck, std_bck = backbone.config[\"mean\"], backbone.config[\"std\"]\n",
    "\n",
    "    print(f\"BACKBONE: {mapping[backbone_configs[backbones.index(backbone)]['name']].upper()}\")\n",
    "    upsample_backbone(backbone, img_batch, model, (mean_bck, std_bck), (mean_ups, std_ups), sizes=[448])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_configs = [\n",
    "    {\"name\": \"vit_base_patch16_dinov3.lvd1689m\"},\n",
    "]\n",
    "backbones, *_ = load_multiple_backbones(cfg, backbone_configs, device)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = backbones[0]\n",
    "backbone.cuda().eval()\n",
    "\n",
    "mean_bck, std_bck = backbone.config[\"mean\"], backbone.config[\"std\"]\n",
    "\n",
    "print(f\"BACKBONE: {backbone_configs[backbones.index(backbone)]['name']}\")\n",
    "upsample_backbone(backbone, img_batch, model, (mean_bck, std_bck), (mean_ups, std_ups), sizes=[64, 128, 256, 512, 1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
